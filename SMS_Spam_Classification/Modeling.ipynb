{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e14676-9a2c-4fd6-8a43-55f793e94a93",
   "metadata": {},
   "source": [
    "# SMS Spam Binary Classification\n",
    "\n",
    "Data is downloaded from Kaggle's [SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "087408c3-77bb-4732-b5db-598e05934d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from torchtext.legacy.data import Field, BucketIterator, TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be6515f-44db-4805-86d2-666eb4eb9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SereneWizard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10b45e3b-5edd-4e2e-9a85-c4993186a3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('.\\dataset\\spam.csv', encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdecdebd-d097-47a8-9934-f53c550e399c",
   "metadata": {},
   "source": [
    "From the glimpse of the dataframe above we identify that we only neeed columns `v1` and `v2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "171ea110-981c-40a7-9d45-0deb39eed5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_not_drop = ['v1', 'v2']\n",
    "data = data.filter(items=to_not_drop, axis=1)\n",
    "# Another way: \n",
    "# data = data.drop(data.columns.difference(to_not_drop), axis=1)\n",
    "new_names = {'v1':'labels', 'v2':'text'}\n",
    "data = data.rename(index=str, columns=new_names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f1f5ef-72d2-4ab7-9ee7-a1931a36a7ba",
   "metadata": {},
   "source": [
    "Splitting the data into training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c81b7043-6979-4c6f-9d8b-50de8146ddc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4457, 2), (1115, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=29)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a90cb-4db9-4f7f-ba1d-ce515fcf045b",
   "metadata": {},
   "source": [
    "Since the index of the `train` and `test` dataset will not be in contiguous order, let's address that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed02257e-f83c-4e94-b818-f3ce76c19500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     labels                                               text\n",
       " 0       ham  Joy's father is John. Then John is the ____ of...\n",
       " 1       ham  Good Morning my Dear........... Have a great &...\n",
       " 2       ham                             Prakesh is there know.\n",
       " 3       ham  IåÕm cool ta luv but v.tired 2 cause i have be...\n",
       " 4       ham            Call me da, i am waiting for your call.\n",
       " ...     ...                                                ...\n",
       " 4452    ham  Aww that's the first time u said u missed me w...\n",
       " 4453    ham      Dude ive been seeing a lotta corvettes lately\n",
       " 4454    ham       I am taking half day leave bec i am not well\n",
       " 4455   spam  8007 25p 4 Alfie Moon's Children in Need song ...\n",
       " 4456   spam  This message is brought to you by GMW Ltd. and...\n",
       " \n",
       " [4457 rows x 2 columns],\n",
       "      labels                                               text\n",
       " 0       ham  I've not called you in a while. This is hoping...\n",
       " 1       ham                  Sorry, I'll call later in meeting\n",
       " 2      spam  Free entry in 2 a weekly comp for a chance to ...\n",
       " 3       ham  Ha! I wouldn't say that I just didn't read any...\n",
       " 4       ham             Thanx but my birthday is over already.\n",
       " ...     ...                                                ...\n",
       " 1110    ham           LOL what happens in Vegas stays in vegas\n",
       " 1111    ham  We walked from my moms. Right on stagwood pass...\n",
       " 1112    ham  I know you are thinkin malaria. But relax, chi...\n",
       " 1113    ham                    Juz go google n search 4 qet...\n",
       " 1114    ham                         K.k:)advance happy pongal.\n",
       " \n",
       " [1115 rows x 2 columns])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.reset_index(drop=True), test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d216191a-469f-4d93-ae59-f2fac000c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('dataset/train.csv', index=False)\n",
    "test.to_csv('dataset/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad943b-9425-465b-aee4-db2e6fecaabe",
   "metadata": {},
   "source": [
    "Next, the text has to be tokenized into the words, and for that, `nltk` library will be used. In this case, `punkt` word tokenizer will be used. This is a standard tokenizer used in `nltk`.   \n",
    "The `Field()` object in `torchtext` allows us to specify how we want the individual text fields to be preprocessed and treated. In this case, `Field()` will allow text to be tokenized into words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9129b91-c106-4e6e-897a-6212e0e14e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.legacy.data.Field(tokenize = word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee90311-dc75-4cc9-929d-e9936e6eb2dd",
   "metadata": {},
   "source": [
    "Similarly, another Field object is defined which will correspond to the labels corresponding to these text messages. Specific Field class called `LabelField()` is used. And it converts the `spam` and `ham` labels to their floating point representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b598cb4-38a3-4abe-8a14-1bd3ad673ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = torchtext.legacy.data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fafa2b99-30fa-47ff-9372-19cfe1e80951",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = [('labels', LABEL), ('text', TEXT)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cdd3c7-e05d-4ead-8902-2118648600f0",
   "metadata": {},
   "source": [
    "These two field objects defined and mapped within `datafields` will know what to do with the data columns when imported.  \n",
    "So, next the raw data that these Field objects will apply to will be imported. For this, `TabularDataset` object is created, that can read from the csv and various other file formats. \n",
    "`split()` function allows splitting the TabularDataset into training and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b58e526-7b19-45f2-9114-0dcd567a8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, tst = torchtext.legacy.data.TabularDataset.splits(path = './dataset', \n",
    "                                                       train = 'train.csv', \n",
    "                                                       test = 'test.csv', \n",
    "                                                       format = 'csv', \n",
    "                                                       skip_header = True,\n",
    "                                                       fields = datafields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db91a2-9048-4bc0-a3fe-1c9f4082a5ca",
   "metadata": {},
   "source": [
    "Let's look at a subset of training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cf60d24-f736-4038-842f-d0094151f95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.legacy.data.example.Example at 0x1a3625b5ac0>,\n",
       " <torchtext.legacy.data.example.Example at 0x1a362d8d430>,\n",
       " <torchtext.legacy.data.example.Example at 0x1a362d8d700>,\n",
       " <torchtext.legacy.data.example.Example at 0x1a362d8dc70>,\n",
       " <torchtext.legacy.data.example.Example at 0x1a3627fe0d0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc31dd-1c2a-47d1-adc6-43e9d947f2d1",
   "metadata": {},
   "source": [
    "We can see that every record in the TabularDataset is an Example object. Further diving deeper, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c8992c5-8b33-462e-a026-c3c0dc3f0c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'text'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[5].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fa7206c-106e-4dad-856b-1ae35f4d0404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sorry', 'i', \"'ve\", 'not', 'gone', 'to', 'that', 'place', '.', 'I.ll', 'do', 'so', 'tomorrow', '.', 'Really', 'sorry', '.']\n"
     ]
    }
   ],
   "source": [
    "print(trn[5].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67dbaa00-73b6-4f59-bbb8-51b201a02866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    }
   ],
   "source": [
    "print(trn[5].labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df87d8af-a28a-4b94-975a-f1dd340b6e77",
   "metadata": {},
   "source": [
    "To look at all the attributes in an Example, `vars()` built-in functino can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eca5d437-db85-4125-b81a-56a78c8f5277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': 'ham', 'text': ['Sorry', 'i', \"'ve\", 'not', 'gone', 'to', 'that', 'place', '.', 'I.ll', 'do', 'so', 'tomorrow', '.', 'Really', 'sorry', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(trn.examples[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620b267-e64e-4c5c-a70c-a4733bcb5822",
   "metadata": {},
   "source": [
    "Next step is to numericalize the representation of the words. The initial numerical representation of the individual words will be done using one-hot encoding.   \n",
    "But when using one-hot encoding, a feature vector to represent a single word will be very large, equaling the size of the vocabulary.   \n",
    "In order to limit the size of the feature vector, we will build a vocabulary on the training data with a maximum size of 105000. That means, we will only consider the top 10500 words of the entire vocabulary. The words outside of this top 10500 will be considered as \"unknown\" words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81ceb5b4-1a5d-4506-af59-3d8be6843595",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn, max_size = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4b88e-7d88-49ea-a4ae-b074c3e72f1d",
   "metadata": {},
   "source": [
    "Training data will be used once again to build a vocabulary for the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea57a2f5-1a51-4c2f-8a58-3386ffa7908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "271e5880-8381-4321-b735-a73ff5dd514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 10002\n",
      "Unique tokens in LaBEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}')\n",
    "print(f'Unique tokens in LaBEL vocabulary: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940ffca-74a4-4a27-b35e-7f1d5f73cae0",
   "metadata": {},
   "source": [
    "Looking at these two vocabularies, the length of the LABEL vocabulary makes sense, but that of the TEXT vocabulary does not. The two extra tokens in the TEXT vocab are as follows: \n",
    "\n",
    "1. \\<unk\\>: Unknown words outside of the top 10000 words. \n",
    "2. \\<pad\\>: Padding to make sentences the same length. \n",
    "\n",
    "The sentences in our text messages can be of different length. But RNN requires that the sentences be of the same length, so we pad the shorter sentences with the \\<pad\\> token. \n",
    "\n",
    "Next, looking at the most common 50 words in the vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4969fb8f-3212-4fff-98d0-aff0254a8ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 3896), ('to', 1695), ('I', 1572), (',', 1484), ('you', 1476), ('?', 1231), ('!', 1128), ('a', 1063), ('the', 948), ('...', 890), ('i', 766), ('&', 745), ('and', 689), ('in', 642), ('is', 632), ('u', 621), (';', 621), ('me', 589), (':', 575), ('for', 531), ('..', 521), ('my', 491), ('it', 481), ('your', 463), ('of', 457), ('have', 415), ('on', 404), ('that', 399), ('2', 394), (')', 389), (\"'s\", 375), ('now', 322), (\"'m\", 317), ('call', 315), ('do', 313), ('are', 309), ('at', 303), ('be', 303), (\"n't\", 301), ('not', 296), ('or', 288), ('U', 286), ('with', 284), ('will', 274), ('can', 270), ('*', 264), ('gt', 261), ('get', 261), ('lt', 259), ('so', 251)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d459974-f059-4af3-ba9b-0f42ac66826f",
   "metadata": {},
   "source": [
    "Even though we had encoded these words using one-hot representation, all we see here is the unique integer IDs assigned to the individual words. This is because pytorch displayed the compact representation of the one-hot encoding, which is the index position of the particular word in the feature vector. \n",
    "\n",
    "We can identify what words belong to these numeric indices by using `itos` mapping. And numeric representation of each string can be identified using `stoi` mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54d6e44a-405e-421e-820b-6a327728c225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '.', 'to', 'I', ',', 'you', '?', '!', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c90105e-f50a-4c95-bf36-7042628e8939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'ham': 0, 'spam': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8dd41-27c2-4eb0-ab7e-cea492a84229",
   "metadata": {},
   "source": [
    "One last preprocessing step to carry out before feeding the data to NN model is to build the batch iterator. Taking the batch size of 64, and using `BucketIterator()`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d18df4b2-85c3-4057-81bf-ed83760f1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_iter, test_iter = torchtext.legacy.data.BucketIterator.splits(\n",
    "    (trn, tst), \n",
    "    batch_size = batch_size, \n",
    "    sort_key = lambda x: len(x.text), \n",
    "    sort_within_batch = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c309959e-153b-426f-b6fb-59a41a9335bf",
   "metadata": {},
   "source": [
    "## Design of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15bfd47e-77df-42c9-8dd6-c69cd587b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c80e2f1-c5d2-48b1-babc-911d66238cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        output, hidden = self.rnn (embedded)\n",
    "        \n",
    "        hidden_1D = hidden.squeeze(0)\n",
    "        \n",
    "        assert torch.equal (output[-1, :, :], hidden_1D)\n",
    "        \n",
    "        return self.fc(hidden_1D)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a837ac-b062-426a-ab53-911fa71b0a0e",
   "metadata": {},
   "source": [
    "Variables into the model:\n",
    "\n",
    "* `input_dim`: gives the size of the feature vectors, which is the one-hot representation of the words. \n",
    "\n",
    "* `embedding_dim`: represents the dimension of the word embeddings - the dense vector representation of the words - that will be trained while training the RNN model. It is a hyperparameter.\n",
    "\n",
    "* `hidden_dim`: is the dimension of the hidden state of the RNN. This is also defined by us. It is also a hyperparameter. \n",
    "\n",
    "* `output_dim`: is the dimension of the output vector (1 for binary classification, since we just need one dimension to represent 0 & 1).\n",
    "\n",
    "And the dimensions of the feed-forward variables: \n",
    "\n",
    "* `text`: \\[sentence_length, batch_size \\]: Every input sentence is a list of indexes of the one-hot encoded words. \n",
    "\n",
    "* `embedded`: \\[sentence_length, batch_size, embedded_dim \\]: The words in each sentence is now represented by its dense embedding. \n",
    "\n",
    "* `ouput`: \\[sentence_length, batch_size, hidden_dim \\]: hidden_dim comes from the concatenation of the hidden state for every time step, i.e. every word. \n",
    "\n",
    "* `hidden`: \\[1, batch_size, hidden_dim \\]: There is one hidden output for each sentence. This final hidden state of the RNN will be fed into the Linear Layer. \n",
    "\n",
    "The unnecessary dimension of the `hidden` is got rid of by using `squeeze()` function. The last hidden state of the output tensor should be equal to the hidden state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4ce32-d535-4a09-ac0d-f7d433631cd8",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d78c89-2404-438e-aa98-44189b1d94e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3e1013f-ae8a-4051-b834-8f2256da9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(TEXT.vocab)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "hidden_dim = 256\n",
    "\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec16d32-c3b5-4585-9ecf-7f7facfb9363",
   "metadata": {},
   "source": [
    "Instantiating the RNN model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5281022-053e-44ce-8272-082ab523aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_dim,\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba6a48-10b5-4879-823b-83d334374455",
   "metadata": {},
   "source": [
    "Model will be trained by using the Adam optimizer, with a learning rate of $10^{-6}$.   \n",
    "The Loss function is Binary Cross-Entropy (BCE) with Logits, which is Cross-Entropy calculation for binary classification + sigmoid function to get predictions in the range of 0 or 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e13a5fe0-db8f-44b0-a53f-a0d050da5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "loss_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d898d7-e4bf-4286-ba10-f087eb000531",
   "metadata": {},
   "source": [
    "Next, building a helper function to run through the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1672f367-80d9-4b1e-a18e-f98ba0ce024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN(model, iterator, optimizer, loss_criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1) # Squeeze because output is [batch_size, 1]\n",
    "        \n",
    "        loss = loss_criterion(predictions, batch.labels)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "        \n",
    "        correct = (rounded_preds == batch.labels).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e27b660-0bcc-421f-82dd-49c75bb412d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.678 | Train Acc: 84.40%\n",
      "| Epoch: 02 | Train Loss: 0.649 | Train Acc: 85.54%\n",
      "| Epoch: 03 | Train Loss: 0.623 | Train Acc: 85.63%\n",
      "| Epoch: 04 | Train Loss: 0.598 | Train Acc: 85.56%\n",
      "| Epoch: 05 | Train Loss: 0.576 | Train Acc: 85.64%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = trainNN(model, train_iter, optimizer, loss_criterion)\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "110d4eda-196d-4560-9b46-64762da94b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = 0\n",
    "epoch_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "131d1d9a-12e6-4a70-b4f7-a9f3d1b47501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(10002, 100)\n",
       "  (rnn): RNN(100, 256)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a83315b0-43b3-42aa-a5ac-75048ccf0966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.576 | Test Acc: 85.64% |\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = loss_criterion(predictions, batch.labels)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "        \n",
    "        correct = (rounded_preds == batch.labels).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "test_loss = epoch_loss / len(test_iter)\n",
    "test_acc = epoch_acc / len(test_iter)\n",
    "\n",
    "print(f'| Test Loss: {train_loss:.3f} | Test Acc: {train_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e6f8c-295f-4ac1-8e60-63f408948474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
